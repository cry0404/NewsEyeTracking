import psycopg
from bs4 import BeautifulSoup, NavigableString
import jieba
import jieba.analyse
import html

class HtmlTextSegmenter:
    def __init__(self):
        self.rows = []

    def load_data(self, db_config, sql):
        with psycopg.connect(**db_config, client_encoding='UTF8') as conn:
            with conn.cursor() as cur:
                cur.execute(sql)
                self.rows = cur.fetchall()
                for i, row in enumerate(self.rows):
                    print(f"Row {i} id={row[0]} type(content)={type(row[1])} type(title)={type(row[2])} type(summary)={type(row[3])} repr content={repr(row[1])[:100]}")

    def segment_text(self, text):
        if not text.strip():
            return text
        tokens = list(jieba.tokenize(text))
        # æŒ‰èµ·å§‹ä½ç½®ä»å¤§åˆ°å°æ’åºï¼Œè¿™æ ·ä»åå¾€å‰æ›¿æ¢ä¸ä¼šå½±å“å‰é¢çš„ä½ç½®
        tokens.sort(key=lambda x: -x[1])
        marked_text = text
        for idx, (word, start, end) in enumerate(tokens):
            # ä¸ºæ¯ä¸ªè¯æ·»åŠ å”¯ä¸€çš„data-id
            span_tag = f'<span data-id="{start}_{end}">{word}</span>'
            marked_text = marked_text[:start] + span_tag + marked_text[end:]
        return marked_text

    def segment_html(self, html_content):
        raw_html = html.unescape(html_content or "")
        soup = BeautifulSoup(raw_html, "html.parser")
        skip_tags = {'script', 'style', 'img', 'iframe'}

        def recurse(node):
            for child in list(node.children):
                if isinstance(child, NavigableString):
                    text = str(child)
                    if text.strip():
                        segmented = self.segment_text(text)
                        # ä½¿ç”¨BeautifulSoupè§£æåˆ†è¯åçš„HTMLï¼Œé¿å…è½¬ä¹‰é—®é¢˜
                        segmented_soup = BeautifulSoup(segmented, "html.parser")
                        child.replace_with(segmented_soup)
                elif child.name not in skip_tags:
                    recurse(child)

        recurse(soup)
        return str(soup)

    def remove_span_tags(self, html_content):
        soup = BeautifulSoup(html.unescape(html_content or ""), "html.parser")
        for span in soup.find_all('span', attrs={"data-id": True}):
            span.unwrap()
        return soup.get_text(separator=" ", strip=True)

    def extract_keywords(self, html_content, title="", summary=""):
        # å»é™¤å·²æœ‰æ ‡ç­¾åæå–çº¯æ–‡æœ¬
        clean_html_text = self.remove_span_tags(html_content)
        clean_title_text = self.remove_span_tags(title)
        clean_summary_text = self.remove_span_tags(summary)

        full_text = ' '.join(filter(None, [clean_title_text, clean_summary_text, clean_html_text]))
        print("[extract_keywords] åˆå¹¶çº¯æ–‡æœ¬:", repr(full_text[:200]))

        keywords = jieba.analyse.extract_tags(full_text, topK=5, withWeight=True)
        print("[extract_keywords] æå–å…³é”®è¯:", keywords)
        return keywords
    
    def should_recommend(self, html_content, title="", summary=""):
        """
        åˆ¤æ–­æ–‡ç« æ˜¯å¦åº”è¯¥è¢«æ¨è
        é™åˆ¶æ¡ä»¶ï¼š
        1. å†…å®¹ä¸èƒ½è¿‡çŸ­ï¼ˆé˜²æ­¢åˆ†è¯å¤±è´¥ï¼‰
        2. å¿…é¡»èƒ½å¤Ÿæå–å‡ºè‡³å°‘5ä¸ªå…³é”®è¯
        """
        # å»é™¤HTMLæ ‡ç­¾è·å–çº¯æ–‡æœ¬
        clean_content = self.remove_span_tags(html_content)
        clean_title = self.remove_span_tags(title)
        clean_summary = self.remove_span_tags(summary)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å†…å®¹
        full_content = ' '.join(filter(None, [clean_title, clean_summary, clean_content]))
        content_length = len(full_content.replace(' ', '').replace('\n', '').replace('\t', ''))
        print(f"[should_recommend] æ€»å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
        
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥ï¼Œé˜²æ­¢å†…å®¹è¿‡çŸ­å¯¼è‡´åˆ†è¯å¤±è´¥
        if content_length < 10:  # é™ä½åˆ°10å­—ç¬¦ï¼Œä»…é˜²æ­¢æç«¯æƒ…å†µ
            print(f"[should_recommend] å†…å®¹è¿‡çŸ­({content_length}å­—ç¬¦)ï¼Œå¯èƒ½å¯¼è‡´åˆ†è¯å¤±è´¥ï¼Œä¸æ¨è")
            return False, "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•åˆ†è¯"
        
        # ä¸»è¦åˆ¤æ–­é€»è¾‘ï¼šæ£€æŸ¥å…³é”®è¯æ•°é‡
        keywords = self.extract_keywords(html_content, title, summary)
        if not keywords or len(keywords) == 0:
            print("[should_recommend] æ— æ³•æå–å…³é”®è¯ï¼Œä¸æ¨è")
            return False, "æ— æ³•æå–å…³é”®è¯"
        
        # æ£€æŸ¥å…³é”®è¯æ•°é‡æ˜¯å¦è¾¾åˆ°è¦æ±‚ï¼ˆè‡³å°‘5ä¸ªï¼‰
        valid_keywords = [kw for kw, weight in keywords if weight > 0.01]  # æƒé‡é˜ˆå€¼
        if len(valid_keywords) < 5:
            print(f"[should_recommend] å…³é”®è¯æ•°é‡ä¸è¶³({len(valid_keywords)}ä¸ª)ï¼Œéœ€è¦è‡³å°‘5ä¸ªå…³é”®è¯ï¼Œä¸æ¨è")
            return False, f"å…³é”®è¯æ•°é‡ä¸è¶³ï¼Œåªæœ‰{len(valid_keywords)}ä¸ª"
        
        print(f"[should_recommend] ç¬¦åˆæ¨èæ¡ä»¶ï¼Œæå–åˆ°{len(valid_keywords)}ä¸ªæœ‰æ•ˆå…³é”®è¯: {valid_keywords}")
        return True, "ç¬¦åˆæ¨èæ¡ä»¶"

    def write_back(self, db_config, table_name, id_column, html_column, keywords_column, title_column, summary_column):
        """
        å°†å¤„ç†åçš„æ•°æ®å†™å›æ•°æ®åº“ï¼Œä½¿ç”¨å•è®°å½•äº‹åŠ¡ç¡®ä¿é²æ£’æ€§
        """
        processed_count = 0
        skipped_count = 0
        error_count = 0
        error_details = []
        
        print(f"\nå¼€å§‹å¤„ç† {len(self.rows)} æ¡è®°å½•...")
        
        for i, row in enumerate(self.rows):
            record_id = None
            try:
                record_id = row[0]
                raw_html = row[1] or ""
                raw_title = row[2] or ""
                raw_summary = row[3] or ""

                print(f"\n[{i+1}/{len(self.rows)}] Processing record id={record_id}")
                
                # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæ¨èæ¡ä»¶
                try:
                    should_recommend, reason = self.should_recommend(raw_html, raw_title, raw_summary)
                    if not should_recommend:
                        print(f"  â†’ Skipping: {reason}")
                        skipped_count += 1
                        continue
                except Exception as e:
                    print(f"  â†’ Error in should_recommend: {e}")
                    error_count += 1
                    error_details.append(f"ID {record_id}: should_recommend failed - {str(e)}")
                    continue

                # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ†è¯
                def has_been_segmented(content):
                    try:
                        if not content:
                            return False
                        soup = BeautifulSoup(html.unescape(content), "html.parser")
                        return bool(soup.find("span", attrs={"data-id": True}))
                    except Exception:
                        return False

                # åˆ†è¯å¤„ç†
                try:
                    new_html = raw_html
                    new_title = raw_title  
                    new_summary = raw_summary
                    
                    if raw_html and not has_been_segmented(raw_html):
                        new_html = self.segment_html(raw_html)
                        print(f"  â†’ HTMLåˆ†è¯å®Œæˆ")
                        
                    if raw_title and not has_been_segmented(raw_title):
                        new_title = self.segment_html(raw_title)
                        print(f"  â†’ æ ‡é¢˜åˆ†è¯å®Œæˆ")
                        
                    if raw_summary and not has_been_segmented(raw_summary):
                        new_summary = self.segment_html(raw_summary)
                        print(f"  â†’ æ‘˜è¦åˆ†è¯å®Œæˆ")
                        
                except Exception as e:
                    print(f"  â†’ Error in segmentation: {e}")
                    error_count += 1
                    error_details.append(f"ID {record_id}: segmentation failed - {str(e)}")
                    continue

                # å…³é”®è¯æå–
                try:
                    keywords_with_weight = self.extract_keywords(raw_html, raw_title, raw_summary)
                    keywords_list = [kw for kw, _ in keywords_with_weight]
                    
                    # é™åˆ¶å…³é”®è¯æ•°é‡ï¼Œé¿å…è¶…è¿‡æ•°æ®åº“å­—æ®µé™åˆ¶
                    if len(keywords_list) > 10:  # æœ€å¤šä¿ç•™10ä¸ªå…³é”®è¯
                        keywords_list = keywords_list[:10]
                        print(f"  â†’ å…³é”®è¯è¢«æˆªæ–­ä¸ºå‰10ä¸ª")
                    
                    # PostgreSQLæ•°ç»„æ ¼å¼å¤„ç†ï¼šå°†keywords_listä¼ ç»™psycopgï¼Œå®ƒä¼šè‡ªåŠ¨è½¬æ¢ä¸ºæ•°ç»„
                    keywords_array = keywords_list  # psycopg3ä¼šè‡ªåŠ¨å¤„ç†Python liståˆ°PostgreSQL arrayçš„è½¬æ¢
                    
                    print(f"  â†’ å…³é”®è¯æå–å®Œæˆ: {keywords_list}")
                    
                except Exception as e:
                    print(f"  â†’ Error in keyword extraction: {e}")
                    error_count += 1
                    error_details.append(f"ID {record_id}: keyword extraction failed - {str(e)}")
                    continue

                # æ•°æ®åº“æ›´æ–°ï¼ˆå•è®°å½•äº‹åŠ¡ï¼‰
                try:
                    with psycopg.connect(**db_config, client_encoding='UTF8') as conn:
                        with conn.cursor() as cur:
                            sql = f"""
                                UPDATE {table_name} SET
                                    {html_column} = %s,
                                    {keywords_column} = %s,
                                    {title_column} = %s,
                                    {summary_column} = %s
                                WHERE {id_column} = %s
                            """
                            
                            # éªŒè¯æ•°æ®é•¿åº¦
                            if len(str(new_html)) > 50000:  # å‡è®¾å­—æ®µæœ€å¤§é•¿åº¦50000å­—ç¬¦
                                print(f"  â†’ Warning: HTMLå†…å®¹è¿‡é•¿ï¼Œå¯èƒ½è¢«æˆªæ–­")
                                new_html = str(new_html)[:49000] + "...[truncated]"
                            
                            cur.execute(sql, (new_html, keywords_array, new_title, new_summary, record_id))
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰è¡Œè¢«æ›´æ–°
                            if cur.rowcount == 0:
                                print(f"  â†’ Warning: æ²¡æœ‰æ‰¾åˆ°IDä¸º {record_id} çš„è®°å½•")
                                error_count += 1
                                error_details.append(f"ID {record_id}: record not found")
                                continue
                            
                            conn.commit()
                            processed_count += 1
                            print(f"  â†’ âœ… æ•°æ®åº“æ›´æ–°æˆåŠŸ")
                            
                except psycopg.Error as db_error:
                    print(f"  â†’ Database error: {db_error}")
                    error_count += 1
                    error_details.append(f"ID {record_id}: database error - {str(db_error)}")
                    continue
                    
                except Exception as e:
                    print(f"  â†’ Error in database update: {e}")
                    error_count += 1
                    error_details.append(f"ID {record_id}: database update failed - {str(e)}")
                    continue
                    
            except Exception as e:
                print(f"  â†’ Unexpected error processing record {record_id}: {e}")
                error_count += 1
                error_details.append(f"ID {record_id}: unexpected error - {str(e)}")
                continue
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        print(f"\n" + "="*60)
        print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡ç»“æœ")
        print(f"="*60)
        print(f"âœ… æˆåŠŸå¤„ç†: {processed_count} æ¡")
        print(f"â­ï¸  è·³è¿‡ï¼ˆä¸ç¬¦åˆæ¡ä»¶ï¼‰: {skipped_count} æ¡")
        print(f"âŒ é”™è¯¯: {error_count} æ¡")
        print(f"ğŸ“ æ€»è®°å½•æ•°: {len(self.rows)} æ¡")
        print(f"ğŸ“Š æˆåŠŸç‡: {(processed_count/len(self.rows)*100):.1f}%" if self.rows else "0%")
        
        if error_details:
            print(f"\nâŒ é”™è¯¯è¯¦æƒ…:")
            for detail in error_details[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
                print(f"  - {detail}")
            if len(error_details) > 10:
                print(f"  ... è¿˜æœ‰ {len(error_details)-10} ä¸ªé”™è¯¯")
        
        print(f"="*60)
        
        return {
            'processed': processed_count,
            'skipped': skipped_count, 
            'errors': error_count,
            'total': len(self.rows),
            'success_rate': processed_count/len(self.rows)*100 if self.rows else 0,
            'error_details': error_details
        }
